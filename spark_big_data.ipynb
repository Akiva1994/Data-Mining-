{"cells":[{"cell_type":"markdown","source":["# Problem Set 3: Spark\n### Instructions: Upload the file to databrcks community edition. Fill the code and text in the designated places and submit a filled .ipynb in moodle by 10/6/2021\n### There are 10 questions overall. Each question is worth 10 points. The questions vary significantly in length and difficulty. \n### You should use apache spark commands, in addition to python code where needed\n### The file contains small functions and other lines of code that will help you solve the exercise. You may copy and/or modify them as you wish\n### Good luck"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b80a97f9-77b8-4ce7-87da-16f625cdfd50"}}},{"cell_type":"markdown","source":["###  Part 1): Finding Prime Numbers with Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91ab1a73-455c-4466-8005-39474feea0c6"}}},{"cell_type":"code","source":["# Class used for timing your commands\nimport time as t\nclass Timer(object):\n    def __init__(self, name=None):\n        self.name = name\n\n    def __enter__(self):\n        self.tstart = time.time()\n\n    def __exit__(self, type, value, traceback):\n        if self.name:\n            print('[%s]' % self.name,)\n        print('Elapsed: %s' % (time.time() - self.tstart))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d72b7f00-e653-43af-9f62-929a47298ea5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Question 1:** Write a **python** function that given a natural number n returns True/False if the number is prime or not by checking its factors (divisors) up to a sqrt(n). <br>\nTest the function on the first 100 natural numbers. <br>\nWhat is the O(f(n)) complexity of this algorithm for finding all primes up to n as a function of n? (assume that arithmatic operations take O(1) regardless of the size of the numbers)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f63e2f39-92a6-4f0c-b234-297363928428"}}},{"cell_type":"code","source":["# SOLUTION\n \ndef check_prime(n):\n   if n > 3:\n      for i in range(2,round(n**0.5)+1):\n          if (n % i) == 0:\n            return False\n      return True\n   else:\n      return True\nx=[0]*100\nfor i in range(0,100):\n  x[i]=check_prime(i)\nprint(x)\n  \n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fff12ab6-c5a2-448e-ac1e-8a4e7d851b23"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[True, True, True, True, False, True, False, True, False, False, False, True, False, True, False, False, False, True, False, True, False, False, False, True, False, False, False, False, False, True, False, True, False, False, False, False, False, True, False, False, False, True, False, True, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, True, False, True, False, False, False, False, False, True, False, False, False, True, False, True, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[True, True, True, True, False, True, False, True, False, False, False, True, False, True, False, False, False, True, False, True, False, False, False, True, False, False, False, False, False, True, False, True, False, False, False, False, False, True, False, False, False, True, False, True, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, True, False, True, False, False, False, False, False, True, False, False, False, True, False, True, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**SOLUTION <br>**\nThe run-time is O(sqrt(n)) for any given n."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"941fb9a5-2409-4bfc-a6a4-4dc2f5c7b4a2"}}},{"cell_type":"markdown","source":["**Question 2:** Create a range object of various sizes starting at 10 up to 10 millions (by a factor of 10, i.e. 10 ,100, 1000 .. etc up to 10 millions) and test your function in (a) in two ways: <br> \n- First, use native python for the range <br>\n- Second, use a Spark RDD for the range <br>\nCompare and print the running times (in seconds, rounded to 2 decimal points) of each methodology (Spark vs. python) for each `n`. <br>\nFor what values of `n` is the RDD is better (we call this order the `breaking point order`) ? Why do you think spark/python is faster before/after? <br>\n*Note:* The answer may vary from run to run and depending on the number of cores available when running using Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5d784f4-af64-4a34-b89b-d54e4b5af2c7"}}},{"cell_type":"code","source":["# SOLUTION\n# PYTHON IMPLEMENTATION:\n#t=Timer(\"t\")\n\nfor j in range(1,8):\n  a = t.time()\n  for i in range(1,10**j):\n    check_prime(i)\n  print(10**j)\n  print(round((t.time() - a),2))\n  print(\"Python\")\n# SPARK SCALA PARALLEL RANGE:\nfor l in range(1,8):\n  a =t.time()\n  interval = sc.parallelize(list(range(1,10**l)))\n  clock = interval.map(lambda x: check_prime(x))\n  print(round((t.time() - a),2))\n  print(10**l)\n  print(\"spark\") "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d63274d2-e679-436c-b605-1ef9183aacab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">10\n0.0\nPython\n100\n0.0\nPython\n1000\n0.0\nPython\n10000\n0.04\nPython\n100000\n0.35\nPython\n1000000\n7.54\nPython\n10000000\n171.61\nPython\n0.13\n10\nspark\n0.02\n100\nspark\n0.02\n1000\nspark\n0.02\n10000\nspark\n0.07\n100000\nspark\n0.32\n1000000\nspark\n2.98\n10000000\nspark\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">10\n0.0\nPython\n100\n0.0\nPython\n1000\n0.0\nPython\n10000\n0.04\nPython\n100000\n0.35\nPython\n1000000\n7.54\nPython\n10000000\n171.61\nPython\n0.13\n10\nspark\n0.02\n100\nspark\n0.02\n1000\nspark\n0.02\n10000\nspark\n0.07\n100000\nspark\n0.32\n1000000\nspark\n2.98\n10000000\nspark\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**SOLUTION** <br>\nAfter examining the results we can see that for small numbers(<100,000) the non-parallel computation is faster. For values larger then 100k we can see a large improvement by using spark. For the largest number that we checked there was a difference of the running time X90. We can assume that for small numbers the function of map-reduction takes longer then the Python method. However for large numbers, the advantage of parallel computing all the numbers at the same time is significantly better, even with taking to account the map-reduce time."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0b4c153-02dc-4902-8283-e8b3ba0c447c"}}},{"cell_type":"markdown","source":["**Question 3:** When splitting the spark range array into different nodes, each nodes will get a consecutive sub-range in standard spark implementations. <br> \nTherefore, it is possible that different nodes will get ranges of numbers of different difficulty (for example, one node may get small, easier numbers, and another may get large, harder numbers). <br>\nThis may cause imbalance between the workload of the nodes and slow down the overall computation. \nHow would you change the Spark implementation such that nodes faster (without changing the function testing primality)? <br>\nImplementing the change in Spark RDD, repeat the computation for the same values of `n` as in the previous question and compare the computation times. <br>\nNote that because we are using the free-tier of databricks, the available cluster only has 1 node, so the actual gain in running time might not be aparent."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26b6fe19-573e-42cf-9a3b-f401d0aa28a5"}}},{"cell_type":"markdown","source":["**SOLUTION** <br>\nTo make the run quicker we would distribute the nodes in a way that the computation time for each nod will be approximately the same. We will implement a random distribution for sending each nod with random numbers."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b568f725-8518-4222-bf68-20d67d9bf37c"}}},{"cell_type":"code","source":["# SOLUTION\nimport time as t\nimport random as r\nfor l in range(1,8):\n  a =t.time()\n  interval = sc.parallelize(list(range(1,10**l)))\n  clock = interval.map(lambda x: check_prime(r.random(x)))\n  print(round((t.time() - a),2))\n  print(10**l)\n  print(\"spark\")\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e549f580-1fa5-41f1-8357-70a1666e341a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">0.01\n10\nspark\n0.01\n100\nspark\n0.01\n1000\nspark\n0.01\n10000\nspark\n0.03\n100000\nspark\n0.29\n1000000\nspark\n2.95\n10000000\nspark\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.01\n10\nspark\n0.01\n100\nspark\n0.01\n1000\nspark\n0.01\n10000\nspark\n0.03\n100000\nspark\n0.29\n1000000\nspark\n2.95\n10000000\nspark\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Question 4:** Write a **python** function that implements Eratosthenes Sieve: given a natural integer `n`, finds all the primes up to that number by iterating over all the numbers larger than `1` in increasing order, and for each number (say i), crossing out all multiples of `i`. <br>\nBy doing so for all the numbers between 2 and `sqrt(n)`, we will end up with only prime numbers. <br>\nTest your function using the same values of `n` from the previous questions with native  python and report the running time in seconds as previously done. What is the `O()` complexity of finding all primes up to `n` as a function of `n`?  how does it compare to the previous method?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd1a733d-71f0-4fcc-baba-df4d91a99134"}}},{"cell_type":"code","source":["def Eratosthenes(n):\n  p = [True for i in range(n + 1)]\n  x = 2\n  p[0]= False\n  p[1]= False \n  while (x**2<= n): \n    if (p[x] == True):\n      for i in range(x * 2, n + 1, x):\n        p[i] = False\n    x+= 1\n  return(p) \n\n\n# RUN AND RECORD TIME\nfor i in range(1,8):\n  a = t.time()\n  Eratosthenes(10**i)\n  print(round((t.time() - a),2))\n  print(10**i)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9745af7e-c418-445d-bdc3-4ad85b989d13"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">0.0\n10\n0.0\n100\n0.0\n1000\n0.0\n10000\n0.03\n100000\n0.17\n1000000\n2.47\n10000000\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.0\n10\n0.0\n100\n0.0\n1000\n0.0\n10000\n0.03\n100000\n0.17\n1000000\n2.47\n10000000\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["*SOLUTION* <BR>\nThe time complexity for the algorithm is O(n*log(log(n))). As we can see in the results  This method is much faster then the Python run-time and faster then even the spark."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed3812c9-9fdf-429b-b722-b0e158035ee1"}}},{"cell_type":"markdown","source":["**Question 5:** Can you implement the algorithm in Question 4 in a parallel implementation using Spark? If not, explain why, if yes, please do so and run for the same values of `n` as in the previous question"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b497411-2fd2-4860-8ee2-687b47997928"}}},{"cell_type":"markdown","source":["*SOLUTION* <br>\nThis algorithm can not be implemented using spark. The reason is thatspark uses paralel computation i.e the data is seperated between different nods, in way we do not controll or know. Since the algorithm is built in way that it starts by deleting all the multiplication of the first primal number that it finds. This would cause many number not to be deleted from the chart. For instance if the number are 50 to 100 all the number in this list will not be deleted besides 100. And same goes for all large numbers."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31049fcf-c8b6-4e64-8ff3-f6c426a77170"}}},{"cell_type":"markdown","source":["###  Part 2): Words Count with Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07d7628f-15b9-4f49-94c5-2e8c471d76ad"}}},{"cell_type":"code","source":["# Reading the book \"war and peace\"\ndbutils.fs.ls(\"dbfs:/FileStore/shared_uploads/akiva.finkelstein@mail.huji.ac.il/war.txt\") # Change to your path\nimport re # Regular expressions\n# Load the \"war and peace\" novel into RDD\nb = sc.textFile('/FileStore/shared_uploads/akiva.finkelstein@mail.huji.ac.il/war.txt')\n\n# A useful function for remiving any non-words and splitting lines into separate words\ndef splitter(line):\n    line = re.sub(r'^\\W+|\\W+$', '', line)\n    return (re.split(r'\\W+', line))\n\nb.take(10) # show first 10 lines"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"107e775c-f685-4f35-9107-bbc0fd0cf69b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[3]: [&#39;&#39;,\n &#39;The Project Gutenberg EBook of War and Peace, by Leo Tolstoy&#39;,\n &#39;&#39;,\n &#39;This eBook is for the use of anyone anywhere at no cost and with almost&#39;,\n &#39;no restrictions whatsoever. You may copy it, give it away or re-use&#39;,\n &#39;it under the terms of the Project Gutenberg License included with this&#39;,\n &#39;eBook or online at www.gutenberg.org&#39;,\n &#39;&#39;,\n &#39;&#39;,\n &#39;Title: War and Peace&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: [&#39;&#39;,\n &#39;The Project Gutenberg EBook of War and Peace, by Leo Tolstoy&#39;,\n &#39;&#39;,\n &#39;This eBook is for the use of anyone anywhere at no cost and with almost&#39;,\n &#39;no restrictions whatsoever. You may copy it, give it away or re-use&#39;,\n &#39;it under the terms of the Project Gutenberg License included with this&#39;,\n &#39;eBook or online at www.gutenberg.org&#39;,\n &#39;&#39;,\n &#39;&#39;,\n &#39;Title: War and Peace&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Question 1:** Upload the \"war and peace\" novel text file and change the path to match your account in the code above. <br> \nCount and print the total number of words and the number of lines in the file (any string separated by spaces is considered a word, even if it is a number, or another non-english-word string)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c0fe326-7921-4767-ac22-4747db2265c3"}}},{"cell_type":"code","source":["# SOLUTION\ndef splitter(line):\n  line= re.sub(r'^\\W+|\\W+$', '',line)\n  return(re.split(r'\\W+',line))\n\nb_split=b.flatMap(lambda x:splitter(x))\nb_split.take(100)\nprint(b_split.count(), \"Number of Words\")\n\n\n#count lines\nb_lines=b.map(lambda x:splitter(x))\nprint(b_lines.count(), \"Number of lines\")\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3be94c51-d463-47e7-910b-c35577004c1e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">590530 Number of Words\n66053 Number of lines\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">590530 Number of Words\n66053 Number of lines\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Question 2:** Compute the number of times each word appears in the file. Ignore case (that is, for example `The` and `the` count as the same word). <br>\nPrint the 10 most frequent words, and the 10 longest words (together with their number of appearances for both)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1199baab-64cd-406a-a1a9-4aeb9f5155c8"}}},{"cell_type":"code","source":["# SOLUTION\nfrom operator import add \nb_lower = b_split.map(lambda x: x.lower())\nhistogram = b_lower.map(lambda x:(x,1)).reduceByKey(add)\nmost_word = histogram.sortBy(lambda x : x[1], ascending = False).take(10)\nlongest_word = histogram.sortBy(lambda x : len(x[0]), ascending = False).take(10)\nprint(most_word)\nprint(longest_word)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc7c0193-3a92-42e2-8ef2-015d27e932f9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[(&#39;the&#39;, 34725), (&#39;and&#39;, 22307), (&#39;to&#39;, 16757), (&#39;of&#39;, 15010), (&#39;&#39;, 13892), (&#39;a&#39;, 10583), (&#39;he&#39;, 10007), (&#39;in&#39;, 9036), (&#39;that&#39;, 8205), (&#39;his&#39;, 7984)]\n[(&#39;characteristically&#39;, 3), (&#39;misunderstandings&#39;, 6), (&#39;unapproachability&#39;, 1), (&#39;superstitiousness&#39;, 1), (&#39;contemporaneously&#39;, 1), (&#39;enthusiastically&#39;, 3), (&#39;circumstantially&#39;, 1), (&#39;misunderstanding&#39;, 6), (&#39;superciliousness&#39;, 1), (&#39;melodramatically&#39;, 1)]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&#39;the&#39;, 34725), (&#39;and&#39;, 22307), (&#39;to&#39;, 16757), (&#39;of&#39;, 15010), (&#39;&#39;, 13892), (&#39;a&#39;, 10583), (&#39;he&#39;, 10007), (&#39;in&#39;, 9036), (&#39;that&#39;, 8205), (&#39;his&#39;, 7984)]\n[(&#39;characteristically&#39;, 3), (&#39;misunderstandings&#39;, 6), (&#39;unapproachability&#39;, 1), (&#39;superstitiousness&#39;, 1), (&#39;contemporaneously&#39;, 1), (&#39;enthusiastically&#39;, 3), (&#39;circumstantially&#39;, 1), (&#39;misunderstanding&#39;, 6), (&#39;superciliousness&#39;, 1), (&#39;melodramatically&#39;, 1)]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Question 3:** Compute the counts of consecutive **pairs** of words in the file. Ignore case. Ignore empty words. <br>\nThe order of words in the pair matters (that is, for example, the pair `she is` should be counted as a different pair form the paier `is she`). <br>\nPrint the 10 most frequent **pairs** of words together with their count"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e263be4-836d-4dca-a866-1fb8863242fa"}}},{"cell_type":"code","source":["# SOLUTION\nwordPairCount1 = b.map(lambda x: x.lower()).map(lambda line: line.split()).flatMap(lambda x: [((x[i], x[i + 1]), 1) for i in range(0, len(x) - 1)]).reduceByKey(lambda a,b:a + b)\nwordPairCount1.sortBy(lambda x:x[1], ascending = False).take(10)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4923893c-0cb9-4e35-8d91-5757ad6c9a54"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[6]: [((&#39;of&#39;, &#39;the&#39;), 3851),\n ((&#39;to&#39;, &#39;the&#39;), 2189),\n ((&#39;in&#39;, &#39;the&#39;), 2174),\n ((&#39;and&#39;, &#39;the&#39;), 1390),\n ((&#39;at&#39;, &#39;the&#39;), 1281),\n ((&#39;on&#39;, &#39;the&#39;), 1236),\n ((&#39;he&#39;, &#39;had&#39;), 1141),\n ((&#39;did&#39;, &#39;not&#39;), 995),\n ((&#39;with&#39;, &#39;a&#39;), 898),\n ((&#39;he&#39;, &#39;was&#39;), 859)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: [((&#39;of&#39;, &#39;the&#39;), 3851),\n ((&#39;to&#39;, &#39;the&#39;), 2189),\n ((&#39;in&#39;, &#39;the&#39;), 2174),\n ((&#39;and&#39;, &#39;the&#39;), 1390),\n ((&#39;at&#39;, &#39;the&#39;), 1281),\n ((&#39;on&#39;, &#39;the&#39;), 1236),\n ((&#39;he&#39;, &#39;had&#39;), 1141),\n ((&#39;did&#39;, &#39;not&#39;), 995),\n ((&#39;with&#39;, &#39;a&#39;), 898),\n ((&#39;he&#39;, &#39;was&#39;), 859)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Question 4:** Repeat the previous question, but this time count word pairs **unordered**. That is, occurances of `she is` and of `is she` should be counted as instances of the same pair. <br>\nWhen printing the top pairs, the two words should be ordered lexicographically (e.g. `is she` for the above example pair)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc4aa703-85bc-49ae-b42b-8f7a76a8df1b"}}},{"cell_type":"code","source":["# SOLUTION\nwordPairCount2 = b.map(lambda x: x.lower()).map(lambda line: line.split()).flatMap(lambda x:[((sorted((x[i], x[i + 1]))[0],sorted((x[i], x[i + 1]))[1]),1) for i in range(0, len(x) - 1)]).reduceByKey(lambda a,b:a + b)\nwordPairCount2.sortBy(lambda x:x[1], ascending = False).take(10)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df5ca5cd-b5f7-494e-9176-138b2b845a42"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[7]: [((&#39;of&#39;, &#39;the&#39;), 3851),\n ((&#39;the&#39;, &#39;to&#39;), 2189),\n ((&#39;in&#39;, &#39;the&#39;), 2174),\n ((&#39;and&#39;, &#39;the&#39;), 1390),\n ((&#39;at&#39;, &#39;the&#39;), 1281),\n ((&#39;on&#39;, &#39;the&#39;), 1236),\n ((&#39;had&#39;, &#39;he&#39;), 1187),\n ((&#39;did&#39;, &#39;not&#39;), 995),\n ((&#39;a&#39;, &#39;with&#39;), 898),\n ((&#39;he&#39;, &#39;was&#39;), 880)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: [((&#39;of&#39;, &#39;the&#39;), 3851),\n ((&#39;the&#39;, &#39;to&#39;), 2189),\n ((&#39;in&#39;, &#39;the&#39;), 2174),\n ((&#39;and&#39;, &#39;the&#39;), 1390),\n ((&#39;at&#39;, &#39;the&#39;), 1281),\n ((&#39;on&#39;, &#39;the&#39;), 1236),\n ((&#39;had&#39;, &#39;he&#39;), 1187),\n ((&#39;did&#39;, &#39;not&#39;), 995),\n ((&#39;a&#39;, &#39;with&#39;), 898),\n ((&#39;he&#39;, &#39;was&#39;), 880)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Question 5:** Get for each word the number of times it appears with the first letter being in upper/lower case, separately, such that each word will have two counts associated with it. <br>\nFor example, for the word `The` count seperately the occurances of `The` (and also, for example `THE`) and the occurances of `the` (and also `tTe`, `tHE` ..). <br>\nNext, filter and keep only words appearing with the first letter being both uppercase and lowercase at least once in the file. Sort these words by their uppercase count / lowercase count ratio. <br>\nFinally, print the 10 words with the **highest** ratio (together with the number of appearances in uppercase and lowercase), and similarly the 10 words with the **lowest** ratio. <br>\nAre the results expected/surprising?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae544e94-9e76-47dc-a7cf-e7928d94667f"}}},{"cell_type":"code","source":["# SOLUTION\nlower_words = b.flatMap(lambda line: splitter(line)).filter(lambda word: word != \"\").map(lambda word: word.lower() if word[0] == word[0].lower() else None)\nlower_counts = lower_words.map(lambda word: (word, 1)).reduceByKey(add)\nupper_words = b.flatMap(lambda line: splitter(line)).filter(lambda word: word != \"\").map(lambda word: word.lower() if word[0] == word[0].upper() else None)\nupper_counts = upper_words.map(lambda word: (word, 1)).reduceByKey(add)\n\n\njoin_counts =  upper_counts.join(lower_counts).map(lambda x: (x,(x[1][0]/x[1][1])))\njoin_counts_h = join_counts.sortBy(lambda x: x[1],ascending= False)\njoin_counts_l = join_counts.sortBy(lambda x: x[1])\n\ni = 1\nprint(\"The Words With the Highest Ratio:\")\nfor word, count in join_counts_h.collect()[0:10]:\n    print(\"{} ) {} : {} \".format(i, word, count))\n    i += 1\n    \n\ni = 1\nprint(\"The Words With the Lowest Ratio:\")\nfor word, count in join_counts_l.collect()[0:10]:\n    print(\"{} ) {} : {} \".format(i, word, count))\n    i += 1\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10bdbd6b-a154-4cff-8d46-4859ebc9e4b2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">The Words With the Highest Ratio:\n1 ) (&#39;i&#39;, (4540, 1)) : 4540.0 \n2 ) (&#39;chapter&#39;, (730, 2)) : 365.0 \n3 ) (&#39;emperor&#39;, (625, 6)) : 104.16666666666667 \n4 ) (&#39;mamma&#39;, (105, 2)) : 52.5 \n5 ) (&#39;papa&#39;, (48, 1)) : 48.0 \n6 ) (&#39;polish&#39;, (42, 1)) : 42.0 \n7 ) (&#39;st&#39;, (40, 1)) : 40.0 \n8 ) (&#39;v&#39;, (38, 1)) : 38.0 \n9 ) (&#39;lord&#39;, (51, 2)) : 25.5 \n10 ) (&#39;oh&#39;, (304, 13)) : 23.384615384615383 \nThe Words With the Lowest Ratio:\n1 ) (&#39;went&#39;, (1, 861)) : 0.0011614401858304297 \n2 ) (&#39;eyes&#39;, (1, 826)) : 0.0012106537530266344 \n3 ) (&#39;thought&#39;, (1, 766)) : 0.0013054830287206266 \n4 ) (&#39;been&#39;, (2, 1474)) : 0.0013568521031207597 \n5 ) (&#39;came&#39;, (1, 682)) : 0.001466275659824047 \n6 ) (&#39;away&#39;, (1, 616)) : 0.0016233766233766235 \n7 ) (&#39;t&#39;, (2, 1157)) : 0.001728608470181504 \n8 ) (&#39;head&#39;, (1, 567)) : 0.001763668430335097 \n9 ) (&#39;way&#39;, (1, 493)) : 0.002028397565922921 \n10 ) (&#39;saw&#39;, (1, 462)) : 0.0021645021645021645 \n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The Words With the Highest Ratio:\n1 ) (&#39;i&#39;, (4540, 1)) : 4540.0 \n2 ) (&#39;chapter&#39;, (730, 2)) : 365.0 \n3 ) (&#39;emperor&#39;, (625, 6)) : 104.16666666666667 \n4 ) (&#39;mamma&#39;, (105, 2)) : 52.5 \n5 ) (&#39;papa&#39;, (48, 1)) : 48.0 \n6 ) (&#39;polish&#39;, (42, 1)) : 42.0 \n7 ) (&#39;st&#39;, (40, 1)) : 40.0 \n8 ) (&#39;v&#39;, (38, 1)) : 38.0 \n9 ) (&#39;lord&#39;, (51, 2)) : 25.5 \n10 ) (&#39;oh&#39;, (304, 13)) : 23.384615384615383 \nThe Words With the Lowest Ratio:\n1 ) (&#39;went&#39;, (1, 861)) : 0.0011614401858304297 \n2 ) (&#39;eyes&#39;, (1, 826)) : 0.0012106537530266344 \n3 ) (&#39;thought&#39;, (1, 766)) : 0.0013054830287206266 \n4 ) (&#39;been&#39;, (2, 1474)) : 0.0013568521031207597 \n5 ) (&#39;came&#39;, (1, 682)) : 0.001466275659824047 \n6 ) (&#39;away&#39;, (1, 616)) : 0.0016233766233766235 \n7 ) (&#39;t&#39;, (2, 1157)) : 0.001728608470181504 \n8 ) (&#39;head&#39;, (1, 567)) : 0.001763668430335097 \n9 ) (&#39;way&#39;, (1, 493)) : 0.002028397565922921 \n10 ) (&#39;saw&#39;, (1, 462)) : 0.0021645021645021645 \n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**SOLUTION** <br>\nThe results are not surprising. In the large ratio list we have words that are usally written in upper case (e.g names). For instance it make sense that 'i' would be in upper case almost all the time, because of English syntax.  In a similar way, notice that the words in the list with low ratio, are rarely in the beginning of a sentence therefore they would almost always not be in upper case. for instance, the word \"eyes\" would not get an upper case, since it does not make sense to start a sentce with eyes. \nIn addition the words in both casses are very common, which causses the raitio to be very high/low."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b674c869-a817-4ae1-9e07-2274f866d4a7"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PS3 SPARK","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1505540796235422}},"nbformat":4,"nbformat_minor":0}
